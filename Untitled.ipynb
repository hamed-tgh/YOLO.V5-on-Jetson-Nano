{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c45fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import (non_max_suppression, scale_coords, check_img_size)\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "import cv2\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, \n",
    "                 weights='yolov5s.pt',  # default YOLOv5 small model\n",
    "                 device='0',            # device to run the model on ('cpu' or 'cuda:0')\n",
    "                 imgsz=(640, 640),     # inference image size\n",
    "                 conf_thres=0.25,      # confidence threshold\n",
    "                 iou_thres=0.45,       # IoU threshold for NMS\n",
    "                 classes=None,         # filter by class\n",
    "                 agnostic_nms=False,   # class-agnostic NMS\n",
    "                 half=False,           # use FP16 half-precision inference\n",
    "                 dnn=False):           # use OpenCV DNN for ONNX inference\n",
    "        self.device = select_device(device)\n",
    "        self.weights = weights\n",
    "        self.model = DetectMultiBackend(weights, device=self.device, dnn=dnn, fp16=half)\n",
    "        self.stride = self.model.stride\n",
    "        self.imgsz = check_img_size(imgsz, s=self.stride)\n",
    "        self.conf_thres = conf_thres\n",
    "        self.iou_thres = iou_thres\n",
    "        self.classes = classes\n",
    "        self.agnostic_nms = agnostic_nms\n",
    "        self.half = half\n",
    "\n",
    "        # Warm up the model for faster subsequent inferences\n",
    "        self.model.warmup(imgsz=(1, 3, *self.imgsz))\n",
    "\n",
    "    def detect(self, img):\n",
    "        \"\"\"\n",
    "        Detect objects in a given image (as a NumPy array).\n",
    "        \n",
    "        Parameters:\n",
    "        - img: The input image as a NumPy array (BGR format as used in OpenCV).\n",
    "\n",
    "        Returns:\n",
    "        - im0: The processed image with detected objects annotated.\n",
    "        \"\"\"\n",
    "        # Pre-process the image\n",
    "        img0 = img.copy()  # original image\n",
    "        img = cv2.resize(img, self.imgsz)  # resize to model input size\n",
    "        img = img.transpose((2, 0, 1))  # HWC to CHW\n",
    "        img = torch.from_numpy(img).to(self.device)\n",
    "        img = img.half() if self.half else img.float()  # Convert to FP16 if half-precision is enabled\n",
    "        img /= 255  # Normalize to range [0, 1]\n",
    "        if len(img.shape) == 3:\n",
    "            img = img[None]  # Add batch dimension\n",
    "\n",
    "        # Inference\n",
    "        pred = self.model(img)\n",
    "\n",
    "        # Apply Non-Max Suppression (NMS)\n",
    "        pred = non_max_suppression(pred, self.conf_thres, self.iou_thres, classes=self.classes, agnostic=self.agnostic_nms)\n",
    "\n",
    "        # Process predictions\n",
    "        annotator = Annotator(img0, line_width=3, example=str(self.model.names))\n",
    "\n",
    "        for det in pred:  # per image\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to original image size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "                # Annotate the image with detection results\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    label = f'{self.model.names[int(cls)]} {conf:.2f}'\n",
    "                    annotator.box_label(xyxy, label, color=colors(int(cls), True))\n",
    "\n",
    "        im0 = annotator.result()  # Get annotated image\n",
    "        return im0\n",
    "\n",
    "# Usage example:\n",
    "# Initialize the detector\n",
    "#detector = ObjectDetector(weights='yolov5s.pt', device='cuda:0')\n",
    "\n",
    "# Load an image (using OpenCV for example)\n",
    "#img = cv2.imread('path/to/your/image.jpg')\n",
    "\n",
    "# Detect objects in the image\n",
    "#processed_image = detector.detect(img)\n",
    "\n",
    "# Display the image using OpenCV (optional)\n",
    "#cv2.imshow(\"Detected Image\", processed_image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import numpy as np\n",
    "from jetson_utils import videoOutput\n",
    "\n",
    "\n",
    "class CSI_Camera:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize instance variables\n",
    "        # OpenCV video capture element\n",
    "        self.video_capture = None\n",
    "        # The last captured image from the camera\n",
    "        self.frame = None\n",
    "        self.grabbed = False\n",
    "        # The thread where the video capture runs\n",
    "        self.read_thread = None\n",
    "        self.read_lock = threading.Lock()\n",
    "        self.running = False\n",
    "\n",
    "    def open(self, gstreamer_pipeline_string):\n",
    "        try:\n",
    "            self.video_capture = cv2.VideoCapture(\n",
    "                gstreamer_pipeline_string, cv2.CAP_GSTREAMER\n",
    "            )\n",
    "            # Grab the first frame to start the video capturing\n",
    "            self.grabbed, self.frame = self.video_capture.read()\n",
    "\n",
    "        except RuntimeError:\n",
    "            self.video_capture = None\n",
    "            print(\"Unable to open camera\")\n",
    "            print(\"Pipeline: \" + gstreamer_pipeline_string)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        if self.running:\n",
    "            print('Video capturing is already running')\n",
    "            return None\n",
    "        # create a thread to read the camera image\n",
    "        if self.video_capture != None:\n",
    "            self.running = True\n",
    "            self.read_thread = threading.Thread(target=self.updateCamera)\n",
    "            self.read_thread.start()\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        # Kill the thread\n",
    "        self.read_thread.join()\n",
    "        self.read_thread = None\n",
    "\n",
    "    def updateCamera(self):\n",
    "        # This is the thread to read images from the camera\n",
    "        while self.running:\n",
    "            try:\n",
    "                grabbed, frame = self.video_capture.read()\n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "                with self.read_lock:\n",
    "                    self.grabbed = grabbed\n",
    "                    self.frame = frame\n",
    "            except RuntimeError:\n",
    "                print(\"Could not read image from camera\")\n",
    "        # FIX ME - stop and cleanup thread\n",
    "        # Something bad happened\n",
    "\n",
    "    def read(self):\n",
    "        with self.read_lock:\n",
    "            frame = self.frame.copy()\n",
    "            grabbed = self.grabbed\n",
    "        return grabbed, frame\n",
    "\n",
    "    def release(self):\n",
    "        if self.video_capture != None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "        # Now kill the thread\n",
    "        if self.read_thread != None:\n",
    "            self.read_thread.join()\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=1920,\n",
    "    capture_height=1080,\n",
    "    display_width=1920,\n",
    "    display_height=1080,\n",
    "    framerate=30,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        \"nvarguscamerasrc sensor-id=%d ! \"\n",
    "        \"video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, framerate=(fraction)%d/1 ! \"\n",
    "        \"nvvidconv flip-method=%d ! \"\n",
    "        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "        % (\n",
    "            sensor_id,\n",
    "            capture_width,\n",
    "            capture_height,\n",
    "            framerate,\n",
    "            flip_method,\n",
    "            display_width,\n",
    "            display_height,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_cameras(yolo_detector):\n",
    "    num = 0 \n",
    "    window_title = \"Dual CSI Cameras\"\n",
    "    left_camera = CSI_Camera()\n",
    "    left_camera.open(\n",
    "        gstreamer_pipeline(\n",
    "            sensor_id=1,\n",
    "            capture_width=1280,\n",
    "            capture_height=720,\n",
    "            flip_method=0,\n",
    "            display_width=1280,\n",
    "            display_height=720,\n",
    "            framerate=30\n",
    "        )\n",
    "    )\n",
    "    left_camera.start()\n",
    "\n",
    "\n",
    "    if left_camera.video_capture.isOpened() :\n",
    "\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                _, left_image = left_camera.read()\n",
    "                processed_image = detector.detect(img)\n",
    "                camera_images = np.hstack((left_image, processed_image))\n",
    "\n",
    "                #END TEST\n",
    "\n",
    "\n",
    "                # Check to see if the user closed the window\n",
    "                # Under GTK+ (Jetson Default), WND_PROP_VISIBLE does not work correctly. Under Qt it does\n",
    "                # GTK - Substitute WND_PROP_AUTOSIZE to detect if window has been closed by user\n",
    "                if cv2.getWindowProperty(window_title, cv2.WND_PROP_AUTOSIZE) >= 0:\n",
    "                    cv2.imshow(window_title, camera_images)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                # This also acts as\n",
    "                keyCode = cv2.waitKey(30) & 0xFF\n",
    "                # Stop the program on the ESC key\n",
    "                if keyCode == 27:\n",
    "                    break\n",
    "                elif keyCode == ord(\"s\"):\n",
    "                    cv2.imwrite(\"//home//pars//Desktop//Backup//CODE//MINE//Left_Images//\"+str(num)+'.png' , left_image)\n",
    "                    print(\"CHAP_CHPASHOD\")\n",
    "                    cv2.imwrite(\"//home//pars//Desktop//Backup//CODE//MINE//Right_Images//\"+str(num)+'.png' , right_image)\n",
    "                    print(\"RAST_CHPASHOD     \" , str(num) )\n",
    "                    num+=1\n",
    "                    \n",
    "        finally:\n",
    "\n",
    "            left_camera.stop()\n",
    "            left_camera.release()\n",
    "            \n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(\"Error: Unable to open both cameras\")\n",
    "        left_camera.stop()\n",
    "        left_camera.release()\n",
    "        \n",
    "\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    detector = ObjectDetector(weights='yolov5s.pt', device='cuda:0')\n",
    "    run_cameras(detector)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
